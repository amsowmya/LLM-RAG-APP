{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.build_rag import RAG\n",
    "from utils import inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = inference.predict_rag(\"about fine tune of llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems there is no specific context provided regarding the fine-tuning of LLaMA 2 (Large Language Model Meta AI). However, in general, fine-tuning a large language model like LLaMA 2 involves several steps:\n",
      "\n",
      "1. **Data Collection and Preparation**: Gather and preprocess a dataset that is relevant to the specific task or domain you want to fine-tune the model for.\n",
      "\n",
      "2. **Model Configuration**: Set up the model configuration parameters, such as learning rate, batch size, number of epochs, and other hyperparameters.\n",
      "\n",
      "3. **Training Environment**: Ensure you have the necessary computational resources, such as GPUs or TPUs, and the appropriate software environment (e.g., PyTorch or TensorFlow).\n",
      "\n",
      "4. **Training Process**: Use the prepared data to train the model. This involves feeding the data into the model, calculating loss, and updating the model weights using backpropagation.\n",
      "\n",
      "5. **Evaluation**: After training, evaluate the model's performance on a validation or test dataset to ensure that it has learned effectively and is not overfitting.\n",
      "\n",
      "6. **Fine-tuning Iterations**: Based on the evaluation, you may need to adjust hyperparameters or the dataset and repeat the fine-tuning process.\n",
      "\n",
      "7. **Deployment**: Once the model is fine-tuned and evaluated, it can be deployed for practical use in applications.\n",
      "\n",
      "If you have more specific aspects of fine-tuning LLaMA 2 in mind, please provide additional details or context.\n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = rag.get_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke('about llama2 finetuning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"E:/2024/GenerativeAI/Langchain/Projects/LLM-RAG-APP/source_data\")\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'E:\\\\2024\\\\GenerativeAI\\\\Langchain\\\\Projects\\\\LLM-RAG-APP\\\\source_data\\\\Llama 2- Open Foundation and Fine-Tuned Chat Models.pdf', 'page': 76}, page_content='A.7 Model Card\\nTable 52 presents a model card (Mitchell et al., 2018; Anil et al., 2023) that summarizes details of the models.\\nModel Details\\nModel Developers Meta AI\\nVariations Llama 2 comes in a range of parameter sizes—7B, 13B, and 70B—as well as\\npretrained and fine-tuned variations.\\nInput Models input text only.\\nOutput Models generate text only.\\nModel Architecture Llama 2 isanauto-regressivelanguagemodelthatusesanoptimizedtransformer\\narchitecture. Thetunedversionsusesupervisedfine-tuning(SFT)andreinforce-\\nmentlearning withhuman feedback(RLHF)to aligntohuman preferencesfor\\nhelpfulness and safety.\\nModel Dates Llama 2 was trained between January 2023 and July 2023.\\nStatus This is a static model trained on an offline dataset. Future versions of the tuned\\nmodels will be released as we improve model safety with community feedback.\\nLicense A custom commercial license is available at: ai.meta.com/resources/\\nmodels-and-libraries/llama-downloads/\\nWhere to send com-\\nmentsInstructions on how to provide feedback or comments on the model can be\\nfound in the model README, or by opening an issue in the GitHub repository\\n(https://github.com/facebookresearch/llama/ ).\\nIntended Use\\nIntended Use Cases Llama 2 is intended for commercial and research use in English. Tuned models\\nare intended for assistant-like chat, whereas pretrained models can be adapted\\nfor a variety of natural language generation tasks.\\nOut-of-Scope Uses Use in any manner that violates applicable laws or regulations (including trade\\ncompliancelaws). UseinlanguagesotherthanEnglish. Useinanyotherway\\nthat is prohibited by the Acceptable Use Policy and Licensing Agreement for\\nLlama 2.\\nHardware and Software (Section 2.2)\\nTraining Factors We usedcustomtraininglibraries, Meta’sResearchSuperCluster, andproduc-\\ntionclustersforpretraining. Fine-tuning,annotation,andevaluationwerealso\\nperformed on third-party cloud compute.\\nCarbon Footprint Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware\\nof type A100-80GB (TDP of 350-400W). Estimated total emissions were 539\\ntCO 2eq, 100% of which were offset by Meta’s sustainability program.\\nTraining Data (Sections 2.1 and 3)\\nOverview Llama 2 was pretrained on 2 trillion tokens of data from publicly available\\nsources. The fine-tuning data includes publicly available instruction datasets, as\\nwellasoveronemillionnewhuman-annotatedexamples. Neitherthepretraining\\nnor the fine-tuning datasets include Meta user data.\\nData Freshness The pretraining data has a cutoff of September 2022, but some tuning data is\\nmore recent, up to July 2023.\\nEvaluation Results\\nSee evaluations for pretraining (Section 2); fine-tuning (Section 3); and safety (Section 4).\\nEthical Considerations and Limitations (Section 5.2)\\nLlama 2 is a new technology that carries risks with use. Testing conducted to date has been in\\nEnglish, and has notcovered, nor could it coverall scenarios. For these reasons, aswith all LLMs,\\nLlama 2’s potential outputs cannot be predicted in advance, and the model may in some instances\\nproduceinaccurateorobjectionableresponsestouserprompts. Therefore,beforedeployingany\\napplications of Llama 2, developers should perform safety testing and tuning tailored to their\\nspecific applications of the model. Please see the Responsible Use Guide available available at\\nhttps://ai.meta.com/llama/responsible-user-guide\\nTable 52: Model card for Llama 2 .\\n77')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[76]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "586"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'E:\\\\2024\\\\GenerativeAI\\\\Langchain\\\\Projects\\\\LLM-RAG-APP\\\\source_data\\\\Llama 2- Open Foundation and Fine-Tuned Chat Models.pdf', 'page': 0}, page_content='Llama 2 : Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗Louis Martin†Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "db = Chroma.from_documents(documents,\n",
    "                            embedding = embeddings,\n",
    "                            persist_directory=\"./vector3\")\n",
    "\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = rag.get_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sowmy\\anaconda3\\envs\\nlp\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "db2 = Chroma(persist_directory='./vector2', embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db2.similarity_search('about fine tuning of llama2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'AzureOpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x0000025567876800>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"provide me training details \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Answer the question based on following content\n",
    "{content}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OPENAI_API_KEY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m AzureChatOpenAI(\n\u001b[0;32m      2\u001b[0m     openai_api_version\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023-06-01-preview\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m#2024-05-13\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     azure_deployment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt4omodel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     azure_endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://semantickernelgpt40t.openai.azure.com/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m----> 5\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[43mOPENAI_API_KEY\u001b[49m\n\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OPENAI_API_KEY' is not defined"
     ]
    }
   ],
   "source": [
    "model = AzureChatOpenAI(\n",
    "    openai_api_version= \"2023-06-01-preview\",  #2024-05-13\n",
    "    azure_deployment=\"gpt4omodel\",\n",
    "    azure_endpoint=\"https://semantickernelgpt40t.openai.azure.com/\",\n",
    "    api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ({'content': retriever, 'question': RunnablePassthrough()}\n",
    "| prompt\n",
    "| model\n",
    "| StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'AzureOpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000001B9C08ACD00>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It looks like the content provided is empty, so I don't have specific information from it to answer your question about pretraining data. However, I can provide a general explanation about pretraining data.\\n\\nPretraining data refers to the large datasets used to initially train machine learning models, particularly deep learning models like neural networks, before they are fine-tuned on more specific tasks. This process is known as pretraining. The idea is to train a model on a broad and diverse dataset so that it can learn general features and patterns. These learned features can then be transferred to more specific tasks where the model is fine-tuned on a smaller, task-specific dataset.\\n\\nIn the context of natural language processing (NLP), for example, pretraining data might consist of vast amounts of text from books, articles, websites, and other sources. Models like GPT-3 are pretrained on such large corpora to understand and generate human-like text. This pretraining allows the model to grasp grammar, facts about the world, and some reasoning abilities, which can then be applied to a variety of downstream tasks such as translation, summarization, and question-answering.\\n\\nWould you like more detailed information on any specific aspect of pretraining data?\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('tell me abut pretraining data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings, AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b5746d281d414e9e895014c038992198'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OPENAI_API_KEY= os.getenv('AZURE_OPENAI_API_KEY')\n",
    "OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "client = AzureChatOpenAI(\n",
    "    openai_api_version= \"2023-06-01-preview\",  #2024-05-13\n",
    "    azure_deployment=\"gpt4omodel\",\n",
    "    azure_endpoint=\"https://semantickernelgpt40t.openai.azure.com/\",\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of India is New Delhi. It serves as the center of government and houses important political institutions, including the offices of the President, Prime Minister, and the Parliament of India.', response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 13, 'total_tokens': 50}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_abc28019ad', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-eb48fb05-ac9f-47c8-b3fa-9ea669e32424-0', usage_metadata={'input_tokens': 13, 'output_tokens': 37, 'total_tokens': 50})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.invoke('what is the capital of India')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "# embeddings = AzureOpenAIEmbeddings(\n",
    "#     deployment='textembeding3small',\n",
    "#     model='text-embedding-3-small',\n",
    "#     openai_api_type='azure',\n",
    "#     azure_endpoint='https://semantickernelgpt40t.openai.azure.com/',\n",
    "#     api_key=OPENAI_API_KEY,\n",
    "# )\n",
    "\n",
    "def get_embedding_model() -> AzureOpenAIEmbeddings:\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        deployment=os.getenv('AZURE_EMBEDDING_DEPLOYMENT'),\n",
    "        model='gpt4omodel',\n",
    "        openai_api_type='azure',\n",
    "        azure_endpoint=os.getenv('AZURE_EMBEDDING_ENDPOINT'),\n",
    "        api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embedding_model()\n",
    "\n",
    "res = embeddings.embed_query(\"How are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
